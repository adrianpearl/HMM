import numpy as np
from ahf_utils import mvGaussian
#from scipy.stats import multivariate_normal
from sklearn.cluster import KMeans

np.set_printoptions(precision=5, linewidth=200, suppress=True)

class HMM:
	
	"""
	Defaults:
	type = ergodic (any state reachable from any other state in 1 step)
		can also be left-right (must start from state 0, state can only increase)
	delta = 0 (states are reachable from any lower state)
		if set to int value, state j is unreachable from state i if j > i + delta
		delta is ignored for ergodic HMM
	"""
	def __init__(self, name, numstates, _type="ergodic", A_delta=0, _feedback=True):
		self.states = numstates
		self.type = _type
		self.feedback = _feedback
		self.superfeed = False
		self.fitted = False
		self.model_name = name
		
		# Initialize lambda (transition matrix A, state distributions B,
		# initial state distribution pi)
		self.A = np.random.random_sample((self.states, self.states))
		self.pi = np.random.random_sample(self.states)
		self.B = None			
		if self.type == "leftright":
			for row in range(0, self.states):
				for col in range(0, self.states):
					if col < row or (col > row + A_delta and A_delta != 0):
						self.A[row][col] = 0
			
			self.pi = np.zeros(self.states)
			self.pi[0] = 1
			
		#normalize rows
		row_sums = self.A.sum(axis=1)
		self.A = self.A / row_sums[:, np.newaxis]
	
	"""
	Class method used to load a previously-created model from saved csv files
	"""
	def loadModel(dir_name):
		pass
					
	def printmodel(self):
		pass
	
	"""
	Input: a single observation
	Output: a 1D numpy array of length self.states containing the log probabilities of 
		the given observation being generated by each of the states
	Must be implemented by subclass
	"""
	def logp_obs_states(self, observation):
		pass
		
	"""
	Input: a single observation
	Output: a 1D numpy array of length self.states containing the probabilities of the 
		given observation being generated by each of the states
	Must be implemented by subclass
	"""
	def p_obs_states(self, observation):
		pass
		
	def logpsequence(self, sequence):
		alpha, beta, ct = self.forward_backward(sequence, True)
		return np.sum(np.log(ct))
		
	"""
	Input:	all training observation sequences, sequence lengths
	Output:	log p(all observation sequences | model)
			= sum for all sequences( log p(sequence | model) )
	"""
	def logps(self, X_train, sequence_lengths):
		assert sum(sequence_lengths) == X_train.shape[0]
		if self.feedback:
			print()
			print("calculating overall model logp")
		
		logps = []
		start = 0
		for i, length in enumerate(sequence_lengths):
			seq = X_train[start : start+length]
			
			logps.append(self.logpsequence(seq))
		
		return logps
	
	"""
	Input: all observation sequences concatenated, vector containing sequence lengths
	Output: a 1D numpy array containing the most probable state for each observation
	"""
	def state_sequence_segmentation(self, X_train, sequence_lengths):
		if self.feedback:
			print()
			print("viterbi algorithm")
		assert sum(sequence_lengths) == X_train.shape[0]
		
		logA = np.log(self.A)
		logpi = np.log(self.pi)
		
		state_sequence = []
		#state_segmentation = np.zeros(len(sequence_lengths), self.states)
		
		seqi = 0
		start = 0
		
		if self.feedback:
			print("state segmentations")
			
		printerval = int(round((sequence_lengths.shape[0] / 20), -2))
		print("printerval: ", printerval)
		
		for i, length in enumerate(sequence_lengths):
			if self.feedback and i%printerval == 0:
				print(i)
			seq = X_train[start : start+length]
		
			delta = np.zeros((seq.shape[0], self.states))
			ancestors = np.zeros((seq.shape[0], self.states))
		
			first_obs = seq[0]
			delta[0] = logpi + self.logp_obs_states(first_obs)
			
			for i in range(1, seq.shape[0]):
				if self.superfeed: print()
				obs = seq[i]
				alpha_obs = np.zeros(self.states)
				ancestors_obs = np.zeros(self.states)
				logp_obs = self.logp_obs_states(obs)
				for state in range(self.states):
					logp = delta[i-1] + logA[:,state]
					if self.superfeed: print(state, logp)
					alpha_obs[state] = np.amax(logp) + logp_obs[state]
					ancestors_obs[state] = np.argmax(logp)
				delta[i] = alpha_obs
				if self.superfeed: print(delta[i])
				ancestors[i] = ancestors_obs
			
			states = [np.argmax(delta[-1])]
			for row in np.flip(ancestors.astype(int), 0)[:-1]:
				states.append(row[states[-1]])
			
			states = states[::-1]
			
			if self.superfeed:
				first = []
				st = -1
				for i, state in enumerate(states):
					if state > st:
						st = state
						first.append(i)
				print(first)
				
			state_sequence = state_sequence + states
						
			#state_counts = np.bincount(np.array(states))
			
			#state_segmentation[i] = state_counts
			seqi = seqi + 1
			start = start + length
		
		return np.array(state_sequence)
	
	"""
	Input: a single observation sequence
	Output: numpy arrays alpha and beta:
		rows=number of observations
		columns=number of states
		alpha(i,j) = p(O1, O2, ..., Oi, state(t=i) = j | model)
		beta(i,j) = p(Oi+1, Oi+2, ..., ON, state(t=i) = j | model)
	"""
	def forward_backward(self, sequence, scaled):
		alpha = np.zeros((sequence.shape[0], self.states))
		alpha[0] = np.multiply(self.pi, self.p_obs_states(sequence[0]))
		
		beta = np.zeros((sequence.shape[0], self.states))
		beta[-1] = np.ones(self.states)
		
		ct = np.zeros(sequence.shape[0])
		ct[0] = 1
		
		self.fwd_bckwd_t(sequence, 1, alpha, beta, ct, scaled)
		
		if self.superfeed:
			print(alpha)
			print(beta)
			print(ct)
		
		return alpha, beta, ct
	
	"""
	Recursive function that populates alpha and beta arrays
	Input:
		seq: the observation sequence
		t: time t (each function call populates alpha[t] and beta[t-1])
		alpha: current version of alpha, populated up to row t-1
		beta: current version of beta, populated from row t to N
		ct_prior: scaling factor used to scale row t-1 of alpha
	Output: no return value, modifies alpha and beta in place
	"""
	def fwd_bckwd_t(self, seq, t, alpha, beta, cts, scaled):
		p_obs = self.p_obs_states(seq[t])
		p_obs = p_obs / np.sum(p_obs)
		
		for state in range(self.states):
			alpha[t][state] = p_obs[state]*np.dot(alpha[t-1], self.A[:,state])
		# Normalize rows of alpha to obtain "scaled" alpha
		cts[t] = 1/np.sum(alpha[t])
		if scaled: 
			alpha[t] = alpha[t] * cts[t]
		
		if t == seq.shape[0] - 1:
			beta[t] = beta[t] * cts[t]
		else:
			self.fwd_bckwd_t(seq, t+1, alpha, beta, cts, scaled)
		
		for state in range(self.states):
			beta[t-1][state] = np.sum([self.A[state][j]*p_obs[j]*beta[t][j] for j in range(self.states)])
		# alpha[t-1] was scaled by a factor of ct[t-1], so scale beta[t-1] by the same factor
		if scaled: 
			beta[t-1] = beta[t-1] * cts[t-1]
		
		return
		
	def reestimate_lambda(self, X_train, sequence_lengths):
		pass
	
	def train(self, X_train, sequence_lengths, max_intervals):
		if self.feedback:
			print()
			print("beginning training")
		logps = []
		for i in range(max_intervals):
			if self.feedback:
				print("training iteration: ", i+1)
				self.printmodel()
			#self.state_sequence_segmentation(X_train, sequence_lengths)
			logp = self.reestimate_lambda(X_train, sequence_lengths)
			logps.append(logp)
			
			print()
			print("**********")
			print("logps: ")
			print(logps)
			print("**********")
			print()
			
			if i > 0:
				difflogp = logps[-2] - logps[-1]
				if difflogp > 0 and difflogp < 1:
					print("convergence")
					break
			self.tocsvs(logps)
		
		self.tocsvs(logps)
		return logps
		
	def tocsvs(self, logposteriors):
		print("saving model")
		np.savetxt("%s/pi.csv" % (self.model_name), self.pi, delimiter=",", fmt="%1.5g")
		np.savetxt("%s/A.csv" % (self.model_name), self.A, delimiter=",", fmt="%1.5g")
		np.savetxt("%s/logps.csv" % (self.model_name), logposteriors, delimiter="\n", fmt="%1.5g")
		self.states_tocsvs()
		
	"""
	Saves each of the state model parameters to a csv
	For gaussian mixture model hmms, each state saves 3 csvs
	Subclasses implement
	"""
	def states_tocsvs(self):
		pass

class GaussianMixtureHMM(HMM):

	def __init__(self, name, numstates, _type="ergodic", A_delta=0, _feedback=True, mixture_components=5):
		super().__init__(name, numstates, _type, A_delta, _feedback)
		self.mixture_components = mixture_components
		
	def logp_obs_states(self, obs):
		logprobs = []
		for state in range(self.states):
			cops = self.B[state][0]
			means = self.B[state][1]
			covs = self.B[state][2]
			
			prob = [cops[i]*mvGaussian(obs, means[i], np.diag(covs[i])) for i in range(len(cops))]
			logprobs.append(np.log(sum(prob)))
		return np.array(logprobs)
		
	def p_obs_states(self, obs):
		probs = []
		for state in range(self.states):
			cops = self.B[state][0]
			means = self.B[state][1]
			covs = self.B[state][2]
						
			prob = [cops[i]*mvGaussian(obs, means[i], np.diag(covs[i])) for i in range(len(cops))]
			probs.append(sum(prob))
		return np.array(probs)
		
	def p_obs_state_comps(self, state, obs):
		cops = self.B[state][0]
		means = self.B[state][1]
		covs = self.B[state][2]
		
		probs = [cops[i]*mvGaussian(obs, means[i], np.diag(covs[i])) for i in range(len(cops))]
		return np.array(probs)
	
	def estimate_state_distributions(self, X_train, sequence_lengths):
		if self.feedback:
			print()
			print("estimating initial state distributions")
			
		self.nFeatures = X_train.shape[1]
		self.B = []
		
		# Component occupation probabilities - initialize to uniform
		cop = np.full((self.mixture_components), 1/self.mixture_components)
		
		# Component covariance matrices - initialize to empirical covariance matrix
		cov = np.diag(X_train.transpose().dot(X_train)/X_train.shape[0])
		cov = np.tile(cov, (self.mixture_components, 1))
				
		for i in range(0, self.states):
		
			"""component means - initialize to random observations"""
			randomobs = np.random.randint(X_train.shape[0], size=self.mixture_components)
			means = X_train[randomobs, :]
			self.B.append((cop, means, cov))
		
		state_sequence = self.state_sequence_segmentation(X_train, sequence_lengths)
		
		if self.feedback:
			print()
			print("using state segmentation for k means")
		
		for state in range(self.states):
			if self.feedback:
				print("state: ", state)
			newcov = np.zeros((self.mixture_components, X_train.shape[1]))
			state_observations = X_train[state_sequence == state]
			kmeans = KMeans(n_clusters=self.mixture_components, random_state=0).fit(state_observations)
			for component in range(self.mixture_components):
				cmean = kmeans.cluster_centers_[component]
				com_obs = state_observations[kmeans.labels_ == component]
				newcov[component] = np.sum(np.square(com_obs - cmean)) / com_obs.shape[0]
				newcov[component] = np.maximum(np.full(X_train.shape[1], 0.01), newcov[component])
			
			cops = np.bincount(kmeans.labels_) / state_observations.shape[0]
			self.B[state] = (cops, kmeans.cluster_centers_, newcov)
			
	"""
	Input:	observation sequences and sequence lengths
	Output:	numpy arrays gamma(observations x number of states x components per state)
			and xi(number of states x number of states)
	gamma[t][i][c] = p(O(t) generated from state i, component c | O sequence, model)
	xi[i][j] = total expected transitions from state i to state j in all sequences
	"""
	def gamma_xi(self, X_train, sequence_lengths):
	
		if self.feedback:
			print()
			print("performing E step")

		assert sum(sequence_lengths) == X_train.shape[0]
		gamma = np.zeros((X_train.shape[0], self.states))
		gammac = np.zeros((X_train.shape[0], self.states, self.mixture_components))
		xi = np.zeros((self.states, self.states))
		logp = 0
		
		if self.feedback:
			print("forward backward - sequence number:")
		start = 0
		
		printerval = int(round((sequence_lengths.shape[0] / 20), -2))
		
		for i, length in enumerate(sequence_lengths):
			if self.feedback and i%printerval == 0:
				print(i)
		
			seq = X_train[start : start+length]
			
			alpha, beta, ct = self.forward_backward(seq, True)
			
			logp -= np.sum(np.log(ct))
			
			p_states = self.p_obs_states(seq[0])
			p_states = p_states/np.sum(p_states)
			
			gamma[start] = np.multiply(alpha[0], beta[0])/ct[0]
			
			for state in range(self.states):
				p_st_comps = self.p_obs_state_comps(state, seq[0])
				"""
				if np.any(np.isnan(p_st_comps)) or np.any(np.isinf(p_st_comps)) or np.any(p_st_comps == 0):
					print("bad probability!")
					print("obs: ", seq[0])
					print("state: ", state)
					print("component probabilities: ", p_st_comps)
				"""
				p_st_comps = p_st_comps/np.sum(p_st_comps)
				gammac[start][state] = (alpha[0][state]*beta[0][state]/ct[0])*p_st_comps
		
			for t in range(1, seq.shape[0]):
				
				gamma[start + t] = np.multiply(alpha[t], beta[t])/ct[t]
				
				obs = seq[t]
			
				for state in range(self.states):
					p_st_comps = self.p_obs_state_comps(state, obs)
					"""
					if np.any(np.isnan(p_st_comps)) or np.any(np.isinf(p_st_comps)) or np.any(p_st_comps == 0):
						print("bad probability!")
						print("obs: ", obs)
						print("state: ", state)
						print("component probabilities: ", p_st_comps)
					"""
					p_st_comps = p_st_comps/np.sum(p_st_comps)
					gammac[start + t][state] = (alpha[t][state]*beta[t][state]/ct[t])*p_st_comps
					
				p_states = self.p_obs_states(obs)
				p_states = p_states/np.sum(p_states)
				betat = beta[t]
				for state in range(self.states):
					# Each contribution to xi is the set of probabilities that a
					# transition occurred between states at this exact time
					xi[state] += np.multiply(np.multiply(p_states, betat), self.A[state])*alpha[t-1][state]
			
			start = start+length
		
		print()
		return gamma, gammac, xi, logp
	
	"""
	Input:	observation sequences, sequence lengths
	Output:	no return value, modifies model transition matrix A and state distributions B
	"""
	def reestimate_lambda(self, X_train, sequence_lengths):
		if self.feedback:
			print()
			print("reestimating lambda")
		assert sum(sequence_lengths) == X_train.shape[0]
		
		# E step of EM (calculating alphas, betas, gammas, xis):
		gamma, gammacomps, sumxi, logp = self.gamma_xi(X_train, sequence_lengths)
		
		if self.feedback:
			print()
			print("performing m step")
		# And M step (rest of this function):
		sumgamma = np.sum(gamma, axis=0)
		
		for state in range(self.states):
			gammastate = gammacomps[:,state,:]
			cops = self.B[state][0]
			means = self.B[state][1]
			covs = self.B[state][2]
			
			sumgammacs = np.sum(gammastate, axis=0)
			
			for c in range(self.mixture_components):
				gammac = gammastate[:,c]
				cmean = np.sum(np.multiply(X_train, gammac[:, np.newaxis]), axis=0) / sumgammacs[c]
				
				diff_sq = np.square(X_train - cmean)
				ccov = np.sum(np.multiply(diff_sq, gammac[:, np.newaxis]), axis=0) / sumgammacs[c]
				
				#lower bound on variances
				ccov = np.maximum(np.full(ccov.shape, 0.01), ccov)
			
				means[c] = cmean
				covs[c] = ccov
			
			cops = sumgammacs/np.sum(gammastate)
		
		if self.feedback:
			print()
			print("gamma: ")
			print(sumgamma)
			print("xi: ")
			print(sumxi)
		
		for i in range(self.states):
			for j in range(self.states):
				self.A[i][j] = sumxi[i][j] / sumgamma[i]
				
		row_sums = self.A.sum(axis=1)
		self.A = self.A / row_sums[:, np.newaxis]
				
		return logp
		
	def states_tocsvs(self):
		for state in range(self.states):
			cops = self.B[state][0]
			means = self.B[state][1]
			covs = self.B[state][2]
			
			np.savetxt("%s/state%d_means.csv" % (self.model_name, state), means, delimiter=",", fmt="%1.5g")
			np.savetxt("%s/state%d_covs.csv" % (self.model_name, state), covs, delimiter=",", fmt="%1.5g")
			np.savetxt("%s/state%d_cops.csv" % (self.model_name, state), cops, delimiter=",", fmt="%1.5g")
			
	def loadModel(name):
		loadA = np.loadtxt(name + "/A.csv", delimiter=",")
		loadpi = np.loadtxt(name + "/pi.csv", delimiter=",")
		loadB = []
		numstates = loadA.shape[0]
		for state in range(numstates):
			means = np.loadtxt("%s/state%d_means.csv" % (name, state), delimiter=",")
			covs = np.loadtxt("%s/state%d_covs.csv" % (name, state), delimiter=",")
			cops = np.loadtxt("%s/state%d_cops.csv" % (name, state), delimiter=",")
			loadB.append((cops, means, covs))
		components = loadB[0][1].shape[0]
		model = GaussianMixtureHMM(name, numstates, mixture_components=components)
		model.A = loadA
		model.B = loadB
		model.pi = loadpi
		return model
			
	def printmodel(self):
		print("current model:")
		print("transition matrix:" )
		print(self.A)
		"""
		print()
		print("state models:")
		print()
		for state in range(self.states):
			sm = self.B[state]
			print("state: ", state)
			print("component occupation probabilities: ")
			print(sm[0])
			print("means: ")
			print(sm[1])
			print("covariances: ")
			print(sm[2])
		"""
				
class MultinomialHMM(HMM):

	def __init__(self, name, numstates, observation_values, _type="ergodic", A_delta=0, _feedback=True):
		super().__init__(name, numstates, _type, A_delta, _feedback)
		self.obsValues = observation_values
		self.B = np.random.random_sample((self.states, self.obsValues))
		
	def logp_obs_states(self, obs):
		return np.log(self.B[:,obs])
		
	def p_obs_states(self, obs):
		return self.B[:,obs]
			
	"""
	Input:	observation sequences and sequence lengths
	Output:	numpy arrays gamma(observations x number of states x components per state)
			and xi(number of states x number of states)
	gamma[t][i][c] = p(O(t) generated from state i, component c | O sequence, model)
	xi[i][j] = total expected transitions from state i to state j in all sequences
	"""
	def gamma_xi(self, X_train, sequence_lengths):
	
		if self.feedback:
			print()
			print("performing E step")

		assert sum(sequence_lengths) == X_train.shape[0]
		
		gamma = np.zeros((X_train.shape[0], self.states))
		xi = np.zeros((self.states, self.states))
		logp = 0
		
		if self.feedback:
			print("sequence start:")
		start = 0
		for i, length in enumerate(sequence_lengths):
			if self.feedback:
				print(start)
		
			seq = X_train[start : start+length]
			
			alpha, beta, ct = self.forward_backward(seq, True)
			
			logp += np.sum(np.log(ct))
			
			gamma[start] = np.multiply(alpha[0], beta[0])/ct[0]
		
			for t in range(1, seq.shape[0]):
				obs = seq[t]
			
				gamma[start + t] = np.multiply(alpha[t], beta[t])/ct[t]
					
				p_states = self.p_obs_states(obs)
				betat = beta[t]
				for state in range(self.states):
					# Each contribution to xi is the set of probabilities that a
					# transition occurred between states at this exact time
					xi[state] += np.multiply(np.multiply(p_states, betat), self.A[state])*alpha[t-1][state]
			
			start = start+length
			
		return gamma, xi, logp
	
	"""
	Input:	observation sequences, sequence lengths
	Output:	no return value, modifies model transition matrix A and state distributions B
	"""
	def reestimate_lambda(self, X_train, sequence_lengths):
		if self.feedback:
			print()
			print("reestimating lambda")
		assert sum(sequence_lengths) == X_train.shape[0]
		
		# E step of EM (calculating alphas, betas, gammas, xis):
		gamma, sumxi, logp = self.gamma_xi(X_train, sequence_lengths)
		
		if self.feedback:
			print()
			print("performing m step")
		# And M step (rest of this function):
		sumgamma = np.sum(gamma, axis=0)
		
		for state in range(self.states):
			gammastate = gamma[:,state]
			
			for val in range(self.obsValues):
				self.B[state][val] = np.sum(gammastate[X_train == val])/np.sum(gammastate)
							
		if self.feedback:
			print()
			print("gamma: ")
			print(sumgamma)
			print("xi: ")
			print(sumxi)
		
		for i in range(self.states):
			for j in range(self.states):
				self.A[i][j] = sumxi[i][j] / sumgamma[i]
				
		return logp
				
		
				
	